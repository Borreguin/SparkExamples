// Sparkâ€™s primary abstraction is a distributed collection of items called a Dataset.
// Datasets can be done from several types of inputs (like HDFS files, or any other different files)
./bin/spark-shell

//creating a Dataset from README.md file 
val textFile = spark.read.textFile("README.md")
// actions using the variable textFile:
textFile.count()
textFile.first()

//create new Dataset using a filter:
var textWithSpark = textFile.filter(line => line.contains("Spark"))
textWithSpark.count()

// Example of operations using map and reduction:
// .map maps a line to an integer value, creating a new Dataset.
// .reduce is called inside this new Dataset to find the largest word count 
textFile.map(line => line.split(" ").size).reduce((a, b) => if (a > b) a else b)

// The latter can be written as: (use: import java.lang.Math)
textFile.map(line => line.split(" ").size).reduce((a, b) => Math.max(a, b))

// using commands of MapReduce sintax (Hadoop) - MapReduce implementation:
// flatmap transforms the textline Dataset into a word Dataset.
// GroupByKey groups words that are similar.
// count counts the number of words that appear inside the word Dataset.
 val wordCounts = textFile.flatMap(line => line.split(" ")).groupByKey(identity).count()
 
// To see the result of this line:
wordCounts.collect()

// ## Spark allows CACHING in memory for having a quick access to the data:
linesWithSpark.cache()
linesWithSpark.count()


// ### CREATING A NEW APPLICATION:
# Your directory layout should look like this
$ find .
.
./build.sbt
./src
./src/main
./src/main/scala
./src/main/scala/SimpleApp.scala

# Package a jar containing your application
$ sbt package
...
[info] Packaging {..}/{..}/target/scala-2.11/simple-project_2.11-1.0.jar

# Use spark-submit to run your application
$ YOUR_SPARK_HOME/bin/spark-submit \
  --class "SimpleApp" \
  --master local[4] \
  target/scala-2.11/simple-project_2.11-1.0.jar
...
Lines with a: 46, Lines with b: 23

 
 
 
 